<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>

<script type="text/front-matter">
  title: "Numerical Optimization"
  description: "Concepts in Constrained Optimization"
  authors:
  - Ghanshyam Chandra: http://ghanshyamchandra74.github.io
  - Karan Jeswani: http://codewithkaranjeswani.github.io/
  affiliations:
  - CDS-IISc: http://cds.iisc.ac.in/
  - CDS-IISc: http://cds.iisc.ac.in/
</script>

<div class="topnav">
    <a class="active" href="index.html">Home</a>
    <a href="scribe.html">Scribe</a>
</div>


\(
\newcommand{\bze}{\boldsymbol{\textbf{0}}}
\newcommand{\bA}{\boldsymbol{\textbf{A}}}
\newcommand{\bb}{\boldsymbol{\textbf{b}}}
\newcommand{\bs}{\boldsymbol{\textbf{s}}}
\newcommand{\bc}{\boldsymbol{\textbf{c}}}
\newcommand{\bx}{\boldsymbol{\textbf{x}}}
\newcommand{\by}{\boldsymbol{\textbf{y}}}
\)

<dt-article>
  <h1>Constrained Optimization and Linear Programming</h1>
  <h2>Introducing concepts from Constrained Optimization</h2>
  <dt-byline></dt-byline>
  <p>The general formulation of the constrained optimization problem is:</p>
  <p>
    \begin{align*}
    && \min_{\bx \in \mathbb{R}^n} f(\bx)\\ \\
    \text{subject to } && c_{i}(\bx) = 0 && i \in \mathcal{E} &&& \text{equality constraint}\\
    && c_{i}(\bx) \geq 0 && i \in \mathcal{I} &&& \text{inequality constraint}
    \end{align*}
    Feasible set:
    \begin{align*}
    \Omega = \{\bx | c_{i}(\bx) = 0, \; i\in \mathcal{E} ,\; c_{i}(\bx) \geq 0 , \; i \in \mathcal{I}\}
    \end{align*}
    For a circle of radius \(c\)
    \begin{align*}
    c_{1}(\bx) = -x_{1}^{2} - x_{2}^{2} +c^{2} , \;\; c_{1}(\bx) \geq 0
    \end{align*}
    \(\Omega :\) all the points inside the circle of radius \(c\) including the perimeter.\\
  </p>
  <p><b>Characterized Solution</b>
  </p>
  <p>
  <ol>
    <li>Global</li>
    <li>Local</li>
  </ol>
  </p>
  <p><b>Optimality Conditions</b>
  </p>
  <p>
  <ol>
    <li>Necessary \(1^{st}\) Order Conditions</li>
    <li>Sufficient \(2^{nd}\) Order Conditions</li>
  </ol>
  </p>
  <p>
    \begin{align*}
    f(\bx^{*}) &lt; f(\bx) &&&\underline{\bx} \in \mathcal{N}(\bx^{*})\\
    &&&\underline{\bx} \in \Omega\\
    &&&\bx \in \Omega\\
    &&&\bx \in \mathcal{N}(\bx^{*})\\
    &&&\bx,\bx^{*} \in \Omega
    \end{align*}
  </p>
  <p><b>Active Set A(x): (Check Definition)</b>
  </p>
  <p>
    \begin{align*}
    A(\bx)=\mathcal{E} \cup \{ i \in \mathcal{I} \hspace{0.2cm} | \hspace{0.2cm} c_{i}(\bx)=0\}
    \end{align*}
    Set of indices for which inequality constraints are active + equal indices.
  </p>
  <p><b>Example 1: Single Equality Constraint</b>
  </p>
  <!-- <p>
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{Circle_Equality.eps}
    \end{figure}
  </p> -->
  <p>
    \begin{align*}
    \min_{\bx \in \mathbb{R}^2} &&& x_{1} + x_{2}\\ \\
    \text{subject to} &&& x_{1}^{2} + x_{2}^{2} - 2 = 0 &&\\ \\
    f(\bx) = x_{1} + x_{2} && \mathcal{I} = \phi \text{ , } \mathcal{E} = \{ 1 \}
    \end{align*}
    At the solution :
    <img src="Circle_Equality.png" width="600" height="500">
    \begin{align*}
    \nabla f \hspace{0.2cm} || \hspace{0.2cm} \nabla c_{1}
    \end{align*}
    \begin{align*}
    \nabla f(\bx^{*}) = \lambda_{1}^{*} \nabla c_{1}(\bx^{*})
    \end{align*}
    Is that always True?
    \begin{align*}
    \bx \in \Omega \text{ , } c_{1}(\bx) = 0
    \end{align*}
    We take a small step \(\bs\)
    \begin{align*}
    c_{1}(\bx+\bs) = 0
    \end{align*}
    Taylor Series approximation:
    \begin{align*}
    &c_{1}(\bx) + \nabla c_{1}(\bx)^{T} \bs = 0\\
    &\text{Since } c_{1}(\bx) = 0 \\
    &\nabla c_{1}(\bx)^{T} = 0
    \end{align*}
    We want decrease in f
    \begin{align*}
    f(\bx+\bs) &&lt; f(\bx)\\
    f(\bx) + \nabla f(\bx)^{T} \bs &&lt; f(\bx)
    \end{align*}
    First Order Conditions:
    \begin{align*}
    &\nabla f(\bx)^{T} &lt; 0\\
    &\nabla c_{1}(\bx)^{T} &lt; 0 \;\; \& \;\; \nabla f(\bx)^{T} &lt; 0
    \end{align*}
    If no \(\bs\) exist that satisfy the above 2 conditions, then \(\bx\) is \(\bx^{*}\)
    <!-- <p>
    \begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{Line_Equality.png}
    \end{figure}
  </p> -->
  <!-- <img src="Line_Equality.png" alt="Line_Equality"> -->
   <img src="Line_Equality.png" width="400" height="400">
    \begin{align*}
    \nabla c_{1} || \nabla f \text{ , } \nabla f(\bx^*) = \lambda_{1} \nabla c_{1}(\bx^*)
    \end{align*}
    Lagrangian Function:
    \begin{align*}
    \mathcal{L}(\bx,\lambda_{1}) = f(\bx) - \lambda_{1} \nabla c_{1}(\bx)\\
    \nabla \mathcal{L} = \nabla f - \lambda_{1} \nabla c_{1} = 0
    \end{align*}
    At the solution \(\bx^{*}\), there is a scalar \(\lambda_{1}^{*}\) such that:
    \begin{align*}
    \nabla_{\bx} \mathcal{L}(\bx^{*},\lambda^{*}) = 0
    \end{align*}
  </p>
  <p><b>Example 2: Single Inequality Constraint</b>
  </p>
  <p>
    \begin{align*}
    \min_{\bx \in \mathbb{R}^2} &&& x_{1} + x_{2} &&&\\ \\
    \text{subject to } &&& 2 - x_{1}^{2} - x_{2}^{2} \geq 0 &&&
    \end{align*}
    \(\bs\) decrease:
    \begin{align*}
    \nabla f(\bx)^{T} \bs &lt; 0 \text{ , } \nabla c^{T}\bs = 0
    \end{align*}
    \(\bs\) feasibility:
    \begin{align*}
    c_{1}(\bx+\bs) \hspace{1.5cm} &\geq 0\\
    c_{1}(\bx) + \nabla c_{1}^{T}(\bx) \bs \hspace{0.2cm} &\geq 0
    \end{align*}
  </p>
  <p><b>Case: 1</b>
  </p>
  <p>
    x is completely inside the feasible region If \nabla f(x) \neq 0, we can set:
    \begin{align*}
    \textit{S} = -\alpha \nabla f(\bx)
    \end{align*}
  </p>
  <p><b>"Steepest Descent Direction"</b>
  </p>
  <p><b>Case: 2</b>
  </p>
  <p>
    \begin{align*}
    c_{1}(\bx) + \nabla c_{1}(\bx)^{T} \bs &\geq 0 \text{ , } c_{1}(\bx) = 0\\
    \nabla c_{1}(\bx)^{T} &\geq 0\\
    \nabla f(\bx)^{T} \bs &&lt; 0
    \end{align*}
    \begin{align*}
    \nabla f || \nabla c_{1} \text{ , } \nabla f = \lambda_{1} \nabla c_{1} \text{ , } \lambda_{1} \geq 0
    \end{align*}
    \begin{align*}
    \lambda_{1} c_{1}(\bx) = 0
    \end{align*}
    If \(c_{1}(\bx) = 0 \) then \(\lambda_{1} > 0\)
    If \(c_{1}(\bx) \neq 0\) then \(\lambda_{1} = 0\)
  </p>
  <p><b>Case: 1</b>
  </p>
  <p>
    \begin{align*}
    \min_{\bx \in \mathbb{R}^m} f(\bx) - \lambda_{1} c_{1}(\bx)\\
    \lambda_{1} > 0\\ \\
    \text{minimizer} \mathcal{L}(x,\lambda_{1})\\ \lambda_{1}^{*} \geq 0\\
    \lambda_{1}^{*}c_{1}(\bx^{*}) = 0
    \end{align*}
  </p>
  <p>
  <ol>
    <li><b> Equality</b></li>
    <li><b> Inequality</b></li>
  </ol>
  </p>
  <p>
    \begin{align*}
    \mathcal{L} = f - \lambda_{1}c_{1} - \lambda_{2}c_{2}
    \end{align*}
    \(\nabla \mathcal{L} = 0\), for some \(\lambda^{*} \geq 0\)
    \begin{align*}
    \lambda_{1}^{*} \geq 0 \\
    \lambda_{2}^{*} \geq 0
    \end{align*}
    Complimentary conditions:
    \begin{align*}
    \lambda_{1}^{*} c_{1}^{*} = 0\\
    \lambda_{2}^{*} c_{2}^{*} = 0
    \end{align*}
    First Order Optimality condition of the KKT Conditions.
    Suppose \(\bx^{*}\) is a local solution \(\&\) f, \(c_{i}\) are smooth \(\&\) Constraints should satisfy same
    "qualification", then there is a Lagrange multiplier vector \(\lambda^{*}\) , with component \(\lambda_{i}^{*}\),
    \(i \in \mathcal{E} \cup \mathcal{I}\) such that the following conditions are satisfied.
    \begin{align*}
    \nabla_{x} \mathcal{L}(x^{*},\lambda^{*}) = 0
    \end{align*}
    \begin{align*}
    c_{i}(x) = 0 &&&\forall \text{ } i\in \mathcal{E}\\
    c_{i}(x) \geq 0 &&&\forall \text{ } i\in \mathcal{I}\\
    \lambda_{i}^{*} \geq 0 &&&\forall \text{ } i\in \mathcal{I}\\
    \lambda_{i}^{*} c_{i}(x^{*}) = 0 &&&\forall \text{ } i\in \mathcal{E} \cup \mathcal{I}
    \end{align*}
  </p>
  <section>
    <h1>Linear Programming</h1>
    <section>
      <h1>Introduction</h1>
      <p>
      <ul>
        <li>Linear programming is a technique for the optimization of a linear objective function, subject to linear
          equality and linear inequality constraints.</li>
        <li>Its feasible region is a convex polytope (a geometric object with flat sides), which is a set defined as
          the intersection of finitely many half spaces (one of the two regions that arise from dividing a Euclidean
          n-dimensional space in two parts), each of which is defined by a linear inequality.</li>
        <li>Its objective function is a real-valued linear function defined on this polyhedron.</li>
        <li>A linear programming algorithm finds a point in the polytope where this function has the smallest (or
          largest) value if such a point exists.</li>
      </ul>
      </p>
    </section>
    <section>
      <h1>Feasible Set (Practical Modelling Considerations)</h1>
      <p>
        Difference between Polyhedra and polytope. <br>

        The feasible set is a polytope, a convex, connected
        set with flat, polygonal faces.<br>

        Look at Linear Programming from NPTEL: Shirish Shivade.<br>

        Todo: Examples of Feasible regions.<br>

        Todo: Definition of a corner point (extreme point).
      </p>
    </section>
    <section>
      <h1>Standard Form</h1>
      <p>
        The Simplex Method requires that the problem is reduced to the following standard form before we apply the
        algorithm.
        \begin{align*}
        \text{min } & \bc^T \bx\\
        \text{subject to } & \bA \bx = \bb \text{ , } \bx \geq \bze
        \end{align*}
        where \(\bc \in \mathbb{R}^n\), \(\bx \in \mathbb{R}^n\), \(\bb \in \mathbb{R}^m\) and \(\bA \in \mathbb{R}^{m
        \times n}\).
        We will be able to convert any kind of Linear Program to this form.<br>

        Here rows of \(\bA\) has the co-efficients of respective variables the constraints, and the \(\bb\) has the
        right
        hand side constant. So we have \(m\) constraints and \(n\) variables. Notice that the standard form only works
        with equality constraints, so <b>slack variables</b> are used to be able to solve problems with inequality
        constraints.
      </p>
    </section>
    <section>
      <h1>Optimality Conditions</h1>
      <p>
        KKT conditions here with LICQ. LICQ not done.<br>

        The Lagrangian function for the problem is given by:
        \begin{align*}
        \mathcal{L}(\bx, \lambda, \bs) = \bc^T \bx - \lambda^T (\bA \bx - \bb) - \bs^T \bx
        \end{align*}
        Here \(\lambda \in \mathbb{R}^m\) is the multiplier for the equality constraints and \(\bs \in \mathbb{R}^n\)
        is
        the
        multiplier for the bounded constraints. <br>

        First order necessary conditions from theory of constrained optimization are wriiten as follows:
        \begin{align*}
        \bA^T\lambda + \bs = \bc\\
        \bA \bx = \bb\\
        \bx \geq \bze\\
        \bs \geq \bze\\
        x_i s_i = 0 \text{ , i = } 1, 2, ..., n
        \end{align*}
      </p>
    </section>
    <section>
      <h1>Dual Problem</h1>
      <p>
        Since the standard form deals only with the minimization problem, we must have a method to convert a
        maximization problem to the standard form. This is done by solving the dual problem. <br>

        The primal and the dual, both have the same solution.
      </p>
    </section>

    <section>
      <h1>Revised Simplex Method</h1>
      <p>
        Why does the solution lie on the extreme points? <br>

        How to go from one corner point to the other. Which is the direction of decrease of objective function. <br>
      </p>
    </section>
    <section>
      <h1>Linear Algebra in Simplex Method</h1>
      <p>

      </p>
    </section>
    <section>
      <h1>Others</h1>
      <p>
      <ol>
        <li>Starting the Simplex Method</li>
        <li>Degenerate solutions and cycling</li>
      </ol>
      </p>
    </section>
    <p>We can also cite <dt-cite key="gregor2015draw"></dt-cite> external publications.</p>
  </section>
</dt-article>

<dt-appendix>
</dt-appendix>

<script type="text/bibliography">
  @article{gregor2015draw,
    title={DRAW: A recurrent neural network for image generation},
    author={Gregor, Karol and Danihelka, Ivo and Graves, Alex and Rezende, Danilo Jimenez and Wierstra, Daan},
    journal={arXivreprint arXiv:1502.04623},
    year={2015},
    url={https://arxiv.org/pdf/1502.04623.pdf}
  }
  </script>